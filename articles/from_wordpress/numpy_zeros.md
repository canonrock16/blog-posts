---
title: ""
emoji: "🤖"
type: "idea" # tech: 技術記事 / idea: アイデア
topics: []
published: false
---

題の通り、ndarray を numpy.zeros([行数,列数])で初期化した際、**[行数,列数]サイズの ndarray のデータを入れるためのメモリ領域は確保されない。**内部的には 0 以外のデータに対してだけ実際にメモリを割り当てている模様。
このため、例えば 100 万 ×100 万のような巨大な ndarray であっても、numpy.zeros()で初期化した場合は、メモリ 32GB や 64GB の PC でも軽快に扱うことが出来る。スパースな行列をメモリに制限のある環境で扱う際に利用できるかもしれない。(素直に scipy.sparse を使えば良い気もするが)

```python
import numpy as np
import sys

# 100万×100万,型はfloat64のndarrayを生成
zero_array = np.zeros([1000000,1000000])

# メモリサイズを確認
sys.getsizeof(zero_array)
# 8000000000112(約8TB)

# メモリ32GBのPCでも軽快に動作する
print(zero_array)
#[[0. 0. 0. ... 0. 0. 0.]
# [0. 0. 0. ... 0. 0. 0.]
# [0. 0. 0. ... 0. 0. 0.]
# ...
# [0. 0. 0. ... 0. 0. 0.]
# [0. 0. 0. ... 0. 0. 0.]
# [0. 0. 0. ... 0. 0. 0.]]

zero_array[0,0] = 1
print(zero_array)
#[[1. 0. 0. ... 0. 0. 0.]
# [0. 0. 0. ... 0. 0. 0.]
# [0. 0. 0. ... 0. 0. 0.]
# ...
# [0. 0. 0. ... 0. 0. 0.]
# [0. 0. 0. ... 0. 0. 0.]
# [0. 0. 0. ... 0. 0. 0.]]
```

numpy.ones()で初期化しようとすると、メモリが足りずプロセスが落ちる。

```python
one_array = np.ones([1000000,1000000])
# プロセス異常終了
```
