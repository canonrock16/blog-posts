---
title: "æ©Ÿæ¢°å­¦ç¿’ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³æ§‹ç¯‰ãƒ„ãƒ¼ãƒ«kedroã®åŸºæœ¬çš„ãªä½¿ã„æ–¹"
emoji: "ğŸ‘»"
type: "tech" # tech: æŠ€è¡“è¨˜äº‹ / idea: ã‚¢ã‚¤ãƒ‡ã‚¢
topics: ["python","machine learning","kedro","pipeline"]
published: false
---

# ã¯ã˜ã‚ã«
### kedroã¨ã¯
æ©Ÿæ¢°å­¦ç¿’ã®ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã€œå‰å‡¦ç†ã€œãƒ¢ãƒ‡ãƒ«ä½œæˆã€œç²¾åº¦æ¤œè¨¼ã¾ã§ã®ä¸€é€£ã®æµã‚Œã‚’ã€å†åˆ©ç”¨å¯èƒ½ãªãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’çµ„ã¿åˆã‚ã›ãŸãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã¨ã—ã¦å®Ÿè¡Œã™ã‚‹ãŸã‚ã®ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã§ã™ã€‚
ã“ã®è¨˜äº‹ã§ã¯ã€kedroã®åŸºæœ¬çš„ãªä½¿ã„æ–¹ã‚’ã€æ©Ÿæ¢°å­¦ç¿’ç•Œéšˆã§ã¯ãŠãªã˜ã¿ã®[Irisãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ](https://archive.ics.uci.edu/ml/datasets/iris)ã¸ã®ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°ã‚’è¡Œã†å‡¦ç†ã‚’é€šã—ã¦è¦‹ã¦ã„ãã¾ã™ã€‚

### æ³¨æ„
ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã‚„ãƒ‡ãƒãƒƒã‚°å®Ÿè¡Œã«é–¢ã—ã¦ã¯pythonã®ä¾å­˜é–¢ä¿‚ç®¡ç†ãƒ„ãƒ¼ãƒ«poetryã‚’ä½¿ç”¨ã—ãŸå ´åˆã®æ‰‹æ³•ã‚’è¨˜è¼‰ã—ã¦ã„ã¾ã™ã€‚pipã‚„conda,pipenvã‚’ä½¿ã£ãŸå ´åˆã®ã‚„ã‚Šæ–¹ã¯å…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã«ã‚ã‹ã‚Šã‚„ã™ãæ›¸ã„ã¦ã‚ã‚‹ã®ã§ã€ã‚³ãƒãƒ³ãƒ‰ã¯ãã¡ã‚‰ã‚’ã”è¦§ãã ã•ã„ã€‚
poetryè‡ªä½“ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã¨ä½¿ã„æ–¹ã¯ç°¡å˜ã«ã¾ã¨ã‚ãŸ[è¨˜äº‹](https://qiita.com/canonrock16/items/f77ee2a2df9be5b8cc37)ãŒã‚ã‚‹ã®ã§ã€ã“ã¡ã‚‰ã‚’ã”è¦§ãã ã•ã„ã€‚

# kedroã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

```shell
# kedroã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã€‚ã‚‚ã—ã‚¨ãƒ©ãƒ¼ãŒå‡ºãŸã‚‰ã€Œã¯ã˜ã‚ã«ã€ã§ç´¹ä»‹ã—ãŸpoetryã®è¨˜äº‹ã®ã€Œæ³¨æ„äº‹é …ã€æ¬„ã‚’è¦‹ã¦ã¿ã¦ãã ã•ã„
poetry add kedro

# ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã‚’ç¢ºèª.ãƒ­ãƒ¼ãƒå­—ã§kedroã¨æ›¸ã‹ã‚ŒãŸã‚¢ã‚¹ã‚­ãƒ¼ã‚¢ãƒ¼ãƒˆãŒå‡ºã¦ãã‚Œã°OKã§ã™ã€‚
poetry run kedro info

# kedroã®å„ç¨®æ©Ÿèƒ½ã‚’ä½¿ç”¨ã™ã‚‹ãŸã‚ã®ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸é”ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã€‚ç§ã®ç’°å¢ƒã§ã¯å®Œäº†ã¾ã§ã«10åˆ†ä»¥ä¸Šã‹ã‹ã£ãŸã®ã§æ°—é•·ã«å¾…ã£ã¦ãã ã•ã„
poetry add "kedro[all]"
```

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆä½œæˆ
kedroå…¬å¼ãŒäº‹å‰ã«ç”¨æ„ã—ã¦ãã‚Œã¦ã„ã‚‹ã€Kedroã‚’ç”¨ã„ã¦Irisãƒ‡ãƒ¼ã‚¿ã®ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°ã‚’è¡Œã†ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚‹ã®ã§ã€ãã‚Œã‚‰ã‚’ä½¿ã£ã¦æ–°è¦ã§ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½œæˆã—ã¾ã™ã€‚
ä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã‚’å®Ÿè¡Œã™ã‚‹ã¨ã€ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆè¨­å®šã®Repository Nameã§æŒ‡å®šã—ãŸåç§°ã®ãƒ•ã‚©ãƒ«ãƒ€ãŒã€ä¸­ã«ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãŒã‚ã‚‹çŠ¶æ…‹ã§ä½œæˆã•ã‚Œã¾ã™ã€‚

```shell

# æ–°è¦ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆä½œæˆ
poetry run kedro new
#ä»¥ä¸‹ã€ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ã«ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆè¨­å®šã‚’ã—ã¦ã„ãã¾ã™

#Project Name:
Getting Started
#Repository Name:
getting-started
#Python Package Name:
iris_test
#Generate Example Pipeline:
y
```

ä»¥ä¸Šã‚’è¡Œã†ã¨ã€ã“ã®ã‚ˆã†ãªæ§‹æˆã®ãƒ•ã‚©ãƒ«ãƒ€ãŒä½œæˆã•ã‚Œã¾ã™ã€‚

```shell
getting-started
â”œâ”€â”€ .coveragerc
â”œâ”€â”€ .gitignore # Prevent staging of unnecessary files to git
â”œâ”€â”€ .ipython # IPython startup scripts
â”‚Â Â  â””â”€â”€ profile_default
â”‚Â Â      â””â”€â”€ startup
â”‚Â Â          â””â”€â”€ 00-kedro-init.py
â”œâ”€â”€ .isort.cfg
â”œâ”€â”€ .kedro.yml # Path to discover project context
â”œâ”€â”€ README.md # Project README
â”œâ”€â”€ conf # Project configuration files
â”‚Â Â  â”œâ”€â”€ README.md
â”‚Â Â  â”œâ”€â”€ base
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ catalog.yml
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ credentials.yml
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ logging.yml
â”‚Â Â  â”‚Â Â  â””â”€â”€ parameters.yml
â”‚Â Â  â””â”€â”€ local
â”œâ”€â”€ data # Local project data (not committed to version control)
â”‚Â Â  â”œâ”€â”€ 01_raw
â”‚Â Â  â”‚Â Â  â””â”€â”€ iris.csv
â”‚Â Â  â”œâ”€â”€ 02_intermediate
â”‚Â Â  â”œâ”€â”€ 03_primary
â”‚Â Â  â”œâ”€â”€ 04_feature
â”‚Â Â  â”œâ”€â”€ 05_model_input
â”‚Â Â  â”œâ”€â”€ 06_models
â”‚Â Â  â”œâ”€â”€ 07_model_output
â”‚Â Â  â””â”€â”€ 08_reporting
â”œâ”€â”€ docs # Project documentation
â”‚Â Â  â””â”€â”€ source
â”‚Â Â      â”œâ”€â”€ conf.py
â”‚Â Â      â””â”€â”€ index.rst
â”œâ”€â”€ kedro_cli.py # A collection of Kedro command line interface (CLI) commands
â”œâ”€â”€ logs # Project output logs (not committed to version control)
â”‚Â Â  â””â”€â”€ journals
â”œâ”€â”€ notebooks # Project related Jupyter notebooks
â”œâ”€â”€ setup.cfg
â””â”€â”€ src # Project source code
    â”œâ”€â”€ iris_test
    â”‚Â Â  â”œâ”€â”€ __init__.py
    â”‚Â Â  â”œâ”€â”€ nodes
    â”‚Â Â  â”‚Â Â  â””â”€â”€ __init__.py
    â”‚Â Â  â”œâ”€â”€ pipeline.py
    â”‚Â Â  â”œâ”€â”€ pipelines
    â”‚Â Â  â”‚Â Â  â”œâ”€â”€ __init__.py
    â”‚Â Â  â”‚Â Â  â”œâ”€â”€ data_engineering
    â”‚Â Â  â”‚Â Â  â”‚Â Â  â”œâ”€â”€ README.md
    â”‚Â Â  â”‚Â Â  â”‚Â Â  â”œâ”€â”€ __init__.py
    â”‚Â Â  â”‚Â Â  â”‚Â Â  â”œâ”€â”€ nodes.py
    â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ pipeline.py
    â”‚Â Â  â”‚Â Â  â””â”€â”€ data_science
    â”‚Â Â  â”‚Â Â      â”œâ”€â”€ README.md
    â”‚Â Â  â”‚Â Â      â”œâ”€â”€ __init__.py
    â”‚Â Â  â”‚Â Â      â”œâ”€â”€ nodes.py
    â”‚Â Â  â”‚Â Â      â””â”€â”€ pipeline.py
    â”‚Â Â  â””â”€â”€ run.py
    â”œâ”€â”€ requirements.txt
    â”œâ”€â”€ setup.py
    â””â”€â”€ tests
        â”œâ”€â”€ __init__.py
        â”œâ”€â”€ pipelines
        â”‚Â Â  â””â”€â”€ __init__.py
        â””â”€â”€ test_run.py
```
ãƒ•ã‚©ãƒ«ãƒ€ç¾¤ã®å†…ã€dataãƒ•ã‚©ãƒ«ãƒ€ã¨logsãƒ•ã‚©ãƒ«ãƒ€ã®ä¸­èº«ã¯`.gitignore`ã«ã‚ˆã£ã¦gitã«åæ˜ ã•ã‚Œãªã„ã‚ˆã†ã«ãªã£ã¦ã„ã¾ã™ã€‚å…ƒã‹ã‚‰å…¥ã£ã¦ã„ã‚‹
getting-started/data/01_raw/iris.csvã ã‘ã¯ignoreã•ã‚Œãªã„ã®ã§ã€ä¸è¦ãªå ´åˆã¯é©å®œå¯¾å‡¦ã—ã¦ãã ã•ã„ã€‚

# vscodeã§ã®ãƒ‡ãƒãƒƒã‚°å®Ÿè¡Œã®æº–å‚™
ä»¥ä¸‹ã®ä¾‹ã¯ã€poetryå°å…¥æ™‚ã«`poetry config virtualenvs.in-project true`ã‚’å®Ÿè¡Œã—ã€ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã«.venvãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒä½œæˆã•ã‚Œã‚‹ã‚ˆã†ã«è¨­å®šã—ãŸå ´åˆã®æ–¹æ³•ã§ã™ã€‚ã“ã®ä¾‹ã§ã¯æœ€çµ‚çš„ã«ä»¥ä¸‹ã®ãƒ•ã‚©ãƒ«ãƒ€æ§‹æˆã¨ãªã‚Šã¾ã™ã€‚

```shell

{poetry run kedro newã‚’è¡Œã£ãŸãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª}
â”œâ”€â”€ .env
â”œâ”€â”€ .venv/...
â”œâ”€â”€ .vscode
        â”œâ”€â”€ launch.json
        â””â”€â”€ settings.json
â”œâ”€â”€ getting-started/...
â”œâ”€â”€ poetry.lock
â””â”€â”€ pyproject.toml
  
```

### .envãƒ•ã‚¡ã‚¤ãƒ«ã®ä½œæˆ

```'.env'
PYTHONPATH=/{ä½œæˆã—ãŸãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®srcãƒ•ã‚©ãƒ«ãƒ€ã¾ã§ã®PATH}:$PYTHONPATH
```

### launch.jsonã®ç·¨é›†
pythonç”¨ã®launch.jsonã‚’é–‹ãã€ä»¥ä¸‹ã‚’è¿½è¨˜ã€‚

```launch.json
   // Use IntelliSense to learn about possible attributes.
    // Hover to view descriptions of existing attributes.
    // For more information, visit: https://go.microsoft.com/fwlink/?linkid=830387
    "version": "0.2.0",
    "configurations": [
        {
            "name": "Python: Kedro Run",
            "type": "python",
            "request": "launch",
            "program": "${workspaceFolder}/getting-started/src/iris_test/run.py",
            "console": "integratedTerminal",
            "cwd": "${workspaceFolder}/getting-started"
        }
    ]
```

### settings.jsonã®ç·¨é›†
```settings.json
{
  "python.pythonPath": ".venv/bin/python"
}
```

### ãƒ‡ãƒãƒƒã‚°å®Ÿè¡Œ
`poetry run kedro new`ã‚’è¡Œã£ãŸãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª(.venvãŒã‚ã‚‹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª)ã§vscodeã‚’é–‹ã‘ã°ãƒ‡ãƒãƒƒã‚°å®Ÿè¡Œã§ãã‚‹ã¯ãšã§ã™ã€‚


# ã‚µãƒ³ãƒ—ãƒ«ã‚³ãƒ¼ãƒ‰ã‚’å‹•ã‹ã—ã¦ã¿ã‚‹

Kedroã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã¯å¤§ãã4ã¤ã®ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‹ã‚‰æ§‹æˆã•ã‚Œã¾ã™ã€‚

| Component  |èª¬æ˜ |
|---|---|
|Data Catalog |Pipelineã®æ§‹ç¯‰ã«ä½¿ç”¨ã•ã‚Œã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®é›†åˆã€‚ãã‚Œãã‚Œã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯`load`ã¨`save`ã®èƒ½åŠ›ã‚’æŒã¡ã€ä¾‹ãˆã°pandas.CSVDataSetã¯CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ã—ãŸã‚Šä¿å­˜ã§ãã‚‹ã€‚ |
| Pipeline |ãƒãƒ¼ãƒ‰ã®é›†åˆã€‚nodeã®ä¾å­˜é–¢ä¿‚ã‚„å®Ÿè¡Œé †åºã‚’ç®¡ç†ã™ã‚‹ã€‚  |
| Node|å‰å‡¦ç†ã‚„å­¦ç¿’ã¨ã„ã£ãŸå‡¦ç†æœ¬ä½“|
| Runner |pipelineã‚’æŒ‡å®šã•ã‚ŒãŸdata catalogã‚’ä½¿ã£ã¦å®Ÿè¡Œã™ã‚‹ã‚‚ã®ã€‚ç¾åœ¨SequentialRunner,ParallelRunner,ThreadRunnerã®3ã¤ãŒã‚ã‚‹ã€‚|

ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆä½œæˆã®éš›ã«`Generate Example Pipeline:y`ã¨ã—ãŸã®ã§ã€Irisãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°ã‚’è¡Œã†ç°¡å˜ãªãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ãŒæ—¢ã«å®Ÿè£…ã•ã‚Œã¦ã„ã¾ã™ã€‚ã¾ãšã¯ã“ã‚Œã‚’å‹•ã‹ã—ã¦ã€å…¨ä½“ã®æ§‹æˆã‚’è¦‹ã¦ã¿ã¾ã—ã‚‡ã†ã€‚


```shell

#getting-startedç›´ä¸‹ã«ã¦
poetry run kedro run
```

å‡¦ç†ãŒå®Œäº†ã—ãŸã‚‰`getting-started/logs/info.log`ã‚’è¦‹ã¦ãã ã•ã„ã€‚è¡Œã‚ã‚ŒãŸå‡¦ç†ã®ãƒ­ã‚°ãŒå‡ºã¦ã„ã¾ã™ã€‚

```info.log

2020-06-07 23:14:40,494 - root - INFO - ** Kedro project iris_test
2020-06-07 23:14:42,602 - kedro.io.data_catalog - INFO - Loading data from `example_iris_data` (CSVDataSet)...
2020-06-07 23:14:42,606 - kedro.io.data_catalog - INFO - Loading data from `params:example_test_data_ratio` (MemoryDataSet)...
2020-06-07 23:14:42,606 - kedro.pipeline.node - INFO - Running node: split_data([example_iris_data,params:example_test_data_ratio]) -> [example_test_x,example_test_y,example_train_x,example_train_y]
2020-06-07 23:14:42,612 - kedro.io.data_catalog - INFO - Saving data to `example_train_x` (MemoryDataSet)...
2020-06-07 23:14:42,613 - kedro.io.data_catalog - INFO - Saving data to `example_train_y` (MemoryDataSet)...
2020-06-07 23:14:42,613 - kedro.io.data_catalog - INFO - Saving data to `example_test_x` (MemoryDataSet)...
2020-06-07 23:14:42,613 - kedro.io.data_catalog - INFO - Saving data to `example_test_y` (MemoryDataSet)...
2020-06-07 23:14:42,614 - kedro.runner.sequential_runner - INFO - Completed 1 out of 4 tasks
2020-06-07 23:14:42,614 - kedro.io.data_catalog - INFO - Loading data from `example_train_x` (MemoryDataSet)...
2020-06-07 23:14:42,614 - kedro.io.data_catalog - INFO - Loading data from `example_train_y` (MemoryDataSet)...
2020-06-07 23:14:42,614 - kedro.io.data_catalog - INFO - Loading data from `parameters` (MemoryDataSet)...
2020-06-07 23:14:42,614 - kedro.pipeline.node - INFO - Running node: train_model([example_train_x,example_train_y,parameters]) -> [example_model]
2020-06-07 23:14:42,932 - kedro.io.data_catalog - INFO - Saving data to `example_model` (MemoryDataSet)...
2020-06-07 23:14:42,932 - kedro.runner.sequential_runner - INFO - Completed 2 out of 4 tasks
2020-06-07 23:14:42,932 - kedro.io.data_catalog - INFO - Loading data from `example_model` (MemoryDataSet)...
2020-06-07 23:14:42,933 - kedro.io.data_catalog - INFO - Loading data from `example_test_x` (MemoryDataSet)...
2020-06-07 23:14:42,933 - kedro.pipeline.node - INFO - Running node: predict([example_model,example_test_x]) -> [example_predictions]
2020-06-07 23:14:42,933 - kedro.io.data_catalog - INFO - Saving data to `example_predictions` (MemoryDataSet)...
2020-06-07 23:14:42,934 - kedro.runner.sequential_runner - INFO - Completed 3 out of 4 tasks
2020-06-07 23:14:42,934 - kedro.io.data_catalog - INFO - Loading data from `example_predictions` (MemoryDataSet)...
2020-06-07 23:14:42,934 - kedro.io.data_catalog - INFO - Loading data from `example_test_y` (MemoryDataSet)...
2020-06-07 23:14:42,934 - kedro.pipeline.node - INFO - Running node: report_accuracy([example_predictions,example_test_y]) -> None
2020-06-07 23:14:42,934 - iris.pipelines.data_science.nodes - INFO - Model accuracy on test set: 100.00%
2020-06-07 23:14:42,935 - kedro.runner.sequential_runner - INFO - Completed 4 out of 4 tasks
2020-06-07 23:14:42,935 - kedro.runner.sequential_runner - INFO - Pipeline execution completed successfully.
```
ã“ã‚Œã ã‘ã§ã¯ä½•ã‚‚ã‚ã‹ã‚‰ãªã„ã®ã§ã€1ã¤1ã¤å‡¦ç†ã‚’è¿½ã£ã¦ã„ãã¾ã™ã€‚

### kedro run
`kedro run`ã‚’å®Ÿè¡Œã™ã‚‹ã¨ã€`src/iris_test/run.py`ãŒã‚­ãƒƒã‚¯ã•ã‚Œã¾ã™ã€‚
ãã®éš›ã«è¡Œã‚ã‚Œã‚‹å‡¦ç†ã¯ä»¥ä¸‹ã®æµã‚Œã«ãªã£ã¦ã„ã¾ã™ã€‚

1. KedroãŒæä¾›ã™ã‚‹load_package_contextãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ç”¨ã„ã¦ã€project_contextã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä½œæˆã™ã‚‹ã€‚ã“ã®éš›ã€å¾Œè¿°ã™ã‚‹`catalog.yml`ã‚„`logging.yml`ã€`parameters.yml`ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ã€`DataCatalog`ã‚‚ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–ã™ã‚‹ã€‚
2. `Pipeline`ã‚’ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–ã™ã‚‹
3. `SequentialRunner `ã‚¯ãƒ©ã‚¹ã‚’ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–ã—ã€`Pipeline`ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã¨`DataCatalog`ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’æ¸¡ã™ã€‚

ã¾ãšãƒ‘ãƒ¼ãƒ„ã‚’ä½œã‚Šã€`SequentialRunner `ã‚¯ãƒ©ã‚¹ã«æ¸¡ã—ã¦å®Ÿè¡Œã€ã¨ã„ã£ãŸæµã‚Œã®ã‚ˆã†ã§ã™ã€‚å„ãƒ‘ãƒ¼ãƒ„ã«ã¤ã„ã¦è¦‹ã¦ã„ãã¾ã™ã€‚

### DataCatalog
DataCatalogã‚’å®šç¾©ã™ã‚‹æ–¹æ³•ã¯2ç¨®é¡ã‚ã‚Šã€1ã¤ã¯ä»Šå®Ÿè¡Œã—ãŸãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã§ã‚‚ç”¨ã„ã¦ã„ã‚‹`catalog.yml`ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ã£ã¦è¨­å®šã‚’è¡Œã†ã‚„ã‚Šæ–¹ã€ã‚‚ã†ä¸€ã¤ã¯APIã‚’ä½¿ã£ã¦è¨­å®šã™ã‚‹ã‚„ã‚Šæ–¹ãŒã‚ã‚Šã¾ã™ã€‚

#### 1.`catalog.yml`ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ã£ã¦è¨­å®šã‚’è¡Œã†

```conf/base/catalog.yml
# Example 1: Loads / saves a CSV file from / to a local file system
example_iris_data:
  type: pandas.CSVDataSet
  filepath: data/01_raw/iris.csv
```
ã“ã®ã‚ˆã†ã«ã€æœ€ä½é™**ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå(example_iris_data)ã€ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—(pandas.CSVDataSet)ã¨ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚ã‚Šã‹(data/01_raw/iris.csv)**ã‚’è¨­å®šã™ã‚Œã°ã€èª­ã¿è¾¼ã‚“ã§DataCatalogã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä½œæˆã—ã¦ãã‚Œã‚‹ã‚ˆã†ã§ã™ã€‚
ã“ã‚Œä»¥å¤–ã«ã‚‚è‰²ã€…ãªè¨­å®šãŒã§ãã€ã„ãã¤ã‹ä¾‹ã‚’ç¤ºã™ã¨ã€
##### S3ã‹ã‚‰CSVãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚“ã§åˆ©ç”¨ã™ã‚‹è¨­å®š

```conf/base/catalog.yml
# Example 4: Loads a CSV file from a specific S3 bucket, using credentials and load arguments
motorbikes:
  type: pandas.CSVDataSet
  filepath: s3://your_bucket/data/02_intermediate/company/motorbikes.csv
  credentials: dev_s3
  load_args:
    sep: ','
    skiprows: 5
    skipfooter: 1
    na_values: ['#NA', NA]
```

##### GCSä¸Šã®ã‚¨ã‚¯ã‚»ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€è¨­å®š
```conf/base/catalog.yml
# Example 6: Loads an excel file from Google Cloud Storage
rockets:
  type: pandas.ExcelDataSet
  filepath: gcs://your_bucket/data/02_intermediate/company/motorbikes.xlsx
  fs_args:
    project: my-project
  credentials: my_gcp_credentials
  save_args:
    sheet_name: Sheet1

```



##### DBã«ã‚¯ã‚¨ãƒªã‚’ç™ºè¡Œã—ã¦å¾—ãŸãƒ‡ãƒ¼ã‚¿ã‚’åˆ©ç”¨ã™ã‚‹è¨­å®š

```conf/base/catalog.yml
# Example 12: Load a SQL table with credentials, a database connection, and applies a SQL query to the table
scooters_query:
  type: pandas.SQLQueryDataSet
  credentials: scooters_credentials
  sql: select * from cars where gear=4
  load_args:
    index_col: [name]
```
...ãªã©ãªã©ã€è‰²ã€…ã¨æŸ”è»Ÿã«è¨­å®šã§ãã‚‹ã‚ˆã†ã§ã™ã€‚
ãƒ‡ãƒ¼ã‚¿ã®å–å¾—ã«ã¯`fsspec`ã¨ã„ã†Pythonã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ä½¿ã£ã¦ã„ã¦ã€

- ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ 
- Hadoop File System (HDFS)
- Amazon S3
- Google Cloud Storage
- HTTP(s): http:// or https:// for reading data directly from HTTP web servers.

ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’å–ã‚Šè¾¼ã‚€ã“ã¨ãŒã§ãã¾ã™ã€‚
ã¾ãŸãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿/ä¿å­˜ã®éš›ã®åŒºåˆ‡ã‚Šæ–‡å­—ã‚’ä½•ã«ã™ã‚‹ã‹ã€æ—¥ä»˜ã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã€åœ§ç¸®ã™ã‚‹ã‹ã—ãªã„ã‹ç­‰ã€…ã‚‚ã“ã“ã§æŒ‡å®šã§ãã¾ã™ã€‚

##### ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ«
ä¾‹ãˆã°æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ã¦ã„ã¦ã€ä»Šæ—¥ã‹ã‚‰è¦‹ã¦éå»30æ—¥é–“ã®ãƒ‡ãƒ¼ã‚¿ã‚’inputã¨ã—ãŸã„æ™‚ãªã©ãŒã‚ã‚‹ã¨æ€ã„ã¾ã™ãŒã€ä»¥ä¸‹ã®ã‚ˆã†ã«è¨­å®šã™ã‚‹ã“ã¨ã§æ—¥æ™‚ã‚’ãƒ•ã‚¡ã‚¤ãƒ«åã«å…¥ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆã§ãã€å®Ÿè¡Œæ™‚ã«ã‚½ãƒ¼ã‚¹ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’æŒ‡å®šã—ã¦å®Ÿè¡Œã§ãã‚‹ã‚ˆã†ã§ã™ã€‚

```conf/base/catalog.yml
cars.csv:
  type: pandas.CSVDataSet
  filepath: data/01_raw/company/cars.csv
  versioned: True

# data/01_raw/company/cars.csv/<version>/cars.csvã®ã‚ˆã†ã«ãƒãƒ¼ã‚¸ãƒ§ãƒ³åˆ¥ã«ç®¡ç†ã•ã‚Œã¦ä¿å­˜ã•ã‚Œã‚‹
```

```shell
# å®Ÿè¡Œ
kedro run --load-version="cars.csv:YYYY-MM-DDThh.mm.ss.sssZ"
```




DataCatalogã®æŒ‡å®šã®ä»•æ–¹ã‚„ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ«ã®è©³ç´°ã¯å…¬å¼ã‚µã‚¤ãƒˆã®[DataCatalogã®ãƒšãƒ¼ã‚¸](https://kedro.readthedocs.io/en/stable/04_user_guide/04_data_catalog.html)ã‚’ã”è¦§ãã ã•ã„ã€‚

#### 2.Code APIã‚’ä½¿ã£ã¦è¨­å®šã‚’è¡Œã†
.pyãƒ•ã‚¡ã‚¤ãƒ«ä¸­ã§DataCatalogã®è¨­å®šã‚’ã™ã‚‹å ´åˆã®ã‚„ã‚Šæ–¹ã§ã™ã€‚
ã„ã¾ã„ã¡ä½¿ã„æ‰€ãŒã‚ã‹ã‚‰ãªã„(ymlãƒ•ã‚¡ã‚¤ãƒ«ã§ã®æŒ‡å®šã§æ¸ˆã‚€ãªã‚‰ãã£ã¡ã®ã»ã†ãŒæ¥½ã§ã¯ï¼Ÿ)ã®ã§ã™ãŒã€Jupyterã‚’ä½¿ã£ã¦ãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ”ãƒ³ã‚°ã—ã¦ã„ã‚‹æ™‚ã¯ã“ã£ã¡ã‚’ä½¿ã†æ„Ÿã˜ã§ã—ã‚‡ã†ã‹ï¼Ÿ
ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ¡ãƒ¢ãƒªã«ä¿å­˜ã™ã‚‹å‡¦ç†ãªã‚“ã‹ã¯Jupyterç­‰ã§ãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ”ãƒ³ã‚°ã—ã¦ã„ã‚‹æ™‚ã§ãªã„ã¨ã‚ªã‚¹ã‚¹ãƒ¡ã§ããªã„ã¨å…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã«è¨˜è¼‰ã•ã‚Œã¦ã„ãŸã®ã§æã‚‰ãCode APIè‡ªä½“ãŒãã†ã„ã†ç”¨é€”ãªã®ã ã¨æ€ã†ã®ã§ã™ãŒ...


```catalog.py
from kedro.io import DataCatalog
from kedro.extras.datasets.pandas import (
    CSVDataSet,
    SQLTableDataSet,
    SQLQueryDataSet,
    ParquetDataSet,
)
from kedro.io import MemoryDataSet

# DataCatalogã®ä½œæˆ
io = DataCatalog(
    {
        "bikes": CSVDataSet(filepath="../data/01_raw/bikes.csv"),
        "cars": CSVDataSet(
            filepath="../data/01_raw/cars.csv", load_args=dict(sep=",")
        ),
        "cars_table": SQLTableDataSet(
            table_name="cars", credentials=dict(con="sqlite:///kedro.db")
        ),
        "scooters_query": SQLQueryDataSet(
            sql="select * from cars where gear=4",
            credentials=dict(con="sqlite:///kedro.db"),
        ),
        "ranked": ParquetDataSet(filepath="ranked.parquet"),
    }
)
# åˆ©ç”¨ã§ãã‚‹ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã‚’ä¸€è¦§è¡¨ç¤ºã™ã‚‹
io.list()

# DataCatalogã«ç™»éŒ²ã—ãŸãƒ‡ãƒ¼ã‚¿ã®å‘¼ã³å‡ºã—
cars = io.load("cars")  # data is now loaded as a DataFrame in 'cars'
gear = cars["gear"].values

# ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ¡ãƒ¢ãƒªã«ä¿å­˜ã™ã‚‹
memory = MemoryDataSet(data=None)
io.add("cars_cache", memory)
io.save("cars_cache", "Memory can store anything.")
io.load("car_cache")
```

### Pipelineã¨Node
ä½¿ç”¨ã™ã‚‹ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®å®šç¾©ã¯ã€`src/iris_test/pipeline.py`ã«æ›¸ã„ã¦ã„ãã¾ã™ã€‚
Irisãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½¿ã£ãŸãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã®ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã¯ä»¥ä¸‹ã®ã‚ˆã†ã«è¨˜è¼‰ã•ã‚Œã¦ã„ã¾ã™ã€‚


```src/iris_test/pipeline.py

def create_pipelines(**kwargs) -> Dict[str, Pipeline]:
    """Create the project's pipeline.

    Args:
        kwargs: Ignore any additional arguments added in the future.

    Returns:
        A mapping from a pipeline name to a ``Pipeline`` object.

    """

    data_engineering_pipeline = de.create_pipeline()
    data_science_pipeline = ds.create_pipeline()

    return {
        "de": data_engineering_pipeline,
        "ds": data_science_pipeline,
        "__default__": data_engineering_pipeline + data_science_pipeline,
    }
```
`data_engineering_pipeline`ã¨`data_science_pipeline`ã®2ã¤ã®ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ãŒã‚ã‚Šã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå®Ÿè¡Œæ™‚ã¯`data_engineering_pipeline`â†’`data_science_pipeline`ã®é †ã§å®Ÿè¡Œã•ã‚Œã¾ã™ã€‚

#### data_engineering_pipeline
data_engineering_pipelineã§è¡Œã‚ã‚Œã‚‹å‡¦ç†ã¯`src/iris_test/pipelines/data_engineering/pipeline.py`ã«æ›¸ã„ã¦ã„ãã¾ã™ã€‚

```src/iris_test/pipelines/data_engineering/pipeline.py
from kedro.pipeline import Pipeline, node
from .nodes import split_data

def create_pipeline(**kwargs):
    return Pipeline(
        [
            node(
                split_data,
                ["example_iris_data", "params:example_test_data_ratio"],
                dict(
                    train_x="example_train_x",
                    train_y="example_train_y",
                    test_x="example_test_x",
                    test_y="example_test_y",
                ),
            )
        ]
    )
```
create_pipelineã®æˆ»ã‚Šå€¤ã¨ã—ã¦ã€nodeãŒå…¥ã£ãŸé…åˆ—ã‚’å¼•æ•°ã«å…¥ã‚ŒãŸPipelineã‚’è¿”å´ã—ã¾ã™ã€‚
nodeã®å¼•æ•°ã«ã¯**å½“è©²nodeã§å®Ÿè¡Œã•ã›ãŸã„é–¢æ•°(split_data)ã€é–¢æ•°ã®å¼•æ•°ã€è¿”å´å€¤**ã‚’æŒ‡å®šã—ã¾ã™ã€‚

##### nodeã§å®Ÿè¡Œã•ã›ãŸã„é–¢æ•°
å„ãƒãƒ¼ãƒ‰ã§å®Ÿè¡Œã•ã›ãŸã„é–¢æ•°ã®ä¸­èº«ã¯`src/iris_test/pipelines/data_engineering/nodes.py`ã«æ›¸ã„ã¦ã„ãã¾ã™ã€‚
pandas.dataframeã¨train/testã®æ¯”ç‡ã‚’å—ã‘å–ã‚Šã€æ¯”ç‡é€šã‚Šã«åˆ†å‰²ã—ãŸdictã‚’è¿”å´ã—ã¾ã™ã€‚
ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã§ã¯å˜ç´”ã«åˆ†å‰²ã‚’è¡Œã£ãŸã ã‘ã§ã™ãŒã€ãƒ¢ãƒ‡ãƒ«é–‹ç™ºã‚’è¡Œã†éš›ã¯å‰å‡¦ç†ã®ãƒãƒ¼ãƒ‰ã‚’åŠ ãˆã¦å‡¦ç†ã™ã‚‹ã“ã¨ã«ãªã‚‹ã¨æ€ã„ã¾ã™ã€‚

```src/iris_test/pipelines/data_engineering/nodes.py
from typing import Any, Dict
import pandas as pd

def split_data(data: pd.DataFrame, example_test_data_ratio: float) -> Dict[str, Any]:
    """Node for splitting the classical Iris data set into training and test
    sets, each split into features and labels.
    The split ratio parameter is taken from conf/project/parameters.yml.
    The data and the parameters will be loaded and provided to your function
    automatically when the pipeline is executed and it is time to run this node.
    """
    data.columns = [
        "sepal_length",
        "sepal_width",
        "petal_length",
        "petal_width",
        "target",
    ]
    classes = sorted(data["target"].unique())
    # One-hot encoding for the target variable
    data = pd.get_dummies(data, columns=["target"], prefix="", prefix_sep="")

    # Shuffle all the data
    data = data.sample(frac=1).reset_index(drop=True)

    # Split to training and testing data
    n = data.shape[0]
    n_test = int(n * example_test_data_ratio)
    training_data = data.iloc[n_test:, :].reset_index(drop=True)
    test_data = data.iloc[:n_test, :].reset_index(drop=True)

    # Split the data to features and labels
    train_data_x = training_data.loc[:, "sepal_length":"petal_width"]
    train_data_y = training_data[classes]
    test_data_x = test_data.loc[:, "sepal_length":"petal_width"]
    test_data_y = test_data[classes]

    # When returning many variables, it is a good practice to give them names:
    return dict(
        train_x=train_data_x,
        train_y=train_data_y,
        test_x=test_data_x,
        test_y=test_data_y,
    )

```

##### nodeã§å®Ÿè¡Œã•ã›ãŸã„é–¢æ•°ã®å¼•æ•°
nodeã«æ¸¡ã—ãŸã„DataCatalogã«ç™»éŒ²ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå(example_iris_data)ã‚„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿(params:example_test_data_ratio)ã‚’æŒ‡å®šã§ãã¾ã™ã€‚ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯ã€`params:`ã¨ã„ã†æ¥é ­è¾ã‚’ã¤ã‘ã‚‹ã¨ã€`parameters.yml`ã«è¨˜è¼‰ã—ãŸãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚“ã§ãã‚Œã¾ã™ã€‚

```conf/base/parameters.yml
example_test_data_ratio: 0.2
example_num_train_iter: 10000
example_learning_rate: 0.01
```
ã“ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯éšå±¤æ§‹é€ ã‚’ä½œã‚‹ã“ã¨ãŒã§ãã€ä»¥ä¸‹ã®ã‚ˆã†ã«ã‚¢ã‚¯ã‚»ã‚¹ãŒå¯èƒ½ã§ã™ã€‚

```python

# parameters.ymlãƒ•ã‚¡ã‚¤ãƒ«
step_size: 1
model_params:
    learning_rate: 0.01
    test_data_ratio: 0.2
    number_of_train_iterations: 10000

# nodeé–¢æ•°å®šç¾©
def train_model(data, model):
    lr = model["learning_rate"]
    test_data_ratio = model["test_data_ratio"]
    iterations = model["number_of_train_iterations"]
    ...

# in pipeline definition
node(
    func=train_model,
    inputs=["input_data", "params:model_params"],
    outputs="output_data",
)
```
ã¾ãŸã€å˜ç´”ã«"parameters"ã¨æ›¸ã„ã¦å…¨ã¦ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ¸¡ã™ã“ã¨ã‚‚å¯èƒ½ã§ã™ã€‚

```python

# parameters.yml
def increase_volume(volume, params):
    step = params["step_size"]
    return volume + step

# in pipeline definition
node(
    func=increase_volume, inputs=["input_volume", "parameters"], outputs="output_volume"
)
```



#### data_science_pipeline
ã“ã¡ã‚‰ã‚‚åŒã˜ãpipelineå®šç¾©ã‚’è¦‹ã¦ã„ãã¾ã™ã€‚ã“ã¡ã‚‰ã¯train_model,predict,report_accuracyã®3ã¤ã®ãƒãƒ¼ãƒ‰ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚
ãƒãƒ¼ãƒ‰ã®ä¸­èº«ã¯`src/iris_test/pipelines/data_science/nodes.py`ã«ã‚ã‚Šã¾ã™ãŒã€å°‘ã€…é•·ãã€ç‰¹åˆ¥ãªã“ã¨ã¯ã—ã¦ã„ãªã„ã®ã§çœç•¥ã—ã¾ã™ã€‚

```src/iris_test/pipelines/data_science/pipeline.py
from kedro.pipeline import Pipeline, node
from .nodes import predict, report_accuracy, train_model

def create_pipeline(**kwargs):
    return Pipeline(
        [
            node(
                train_model,
                ["example_train_x", "example_train_y", "parameters"],
                "example_model",
            ),
            node(
                predict,
                dict(model="example_model", test_x="example_test_x"),
                "example_predictions",
            ),
            node(report_accuracy, ["example_predictions", "example_test_y"], None),
        ]
    )
```

### Runner
DataCatalogã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã¨Pipelineã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ãŒã§ããŸã‚‰ã€ã„ã‚ˆã„ã‚ˆRunnerã«æ¸¡ã—ã¦å‡¦ç†ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚ç¾æ™‚ç‚¹ã§ä½¿ç”¨ã§ãã‚‹Runnerã¯3ç¨®é¡ã‚ã‚Šã¾ã™ã€‚ã¾ãšã€ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã§ä½¿ç”¨ã•ã‚Œã¦ã„ã‚‹SequentialRunnerã‹ã‚‰è¦‹ã¦ã„ãã¾ã™ã€‚

#### SequentialRunner
é †æ¬¡ãƒãƒ¼ãƒ‰ã‚’å®Ÿè¡Œã—ã¦ã„ãrunnerã§ã™ã€‚`kedro run`ã«ä½•ã‚‚ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’ä»˜ã‘ãšã«å®Ÿè¡Œã™ã‚‹ã¨SequentialRunnerã§å‡¦ç†ãŒå®Ÿè¡Œã•ã‚Œã¾ã™ã€‚

#### ParallelRunner
ãƒãƒ¼ãƒ‰ã‚’ä¸¦åˆ—ã«å®Ÿè¡Œã—ã¦ã„ãrunnerã§ã™ã€‚`kedro run --parallel`ã¨ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’ä»˜ã‘ã‚‹ã¨ParallelRunnerã§å‡¦ç†ãŒå®Ÿè¡Œã•ã‚Œã¾ã™ã€‚
â€ä¸¦åˆ—ã«â€ã¨ã„ã†ã®ã¯ã€ã¾ãšPipelineã®å„ãƒãƒ¼ãƒ‰ã®ä¾å­˜é–¢ä¿‚ã‚’èª­ã¿è¾¼ã¿ã€ãƒˆãƒãƒ­ã‚¸ã‚«ãƒ«ã‚½ãƒ¼ãƒˆã‚’ä½¿ã£ã¦ä¸¦ã¹æ›¿ãˆã€ä¸¦åˆ—åŒ–ã§ãã‚‹ã¨ã“ã‚(ä¾å­˜é–¢ä¿‚ãŒãªã„ã¨ã“ã‚)ã¯åŒæ™‚ã«å®Ÿè¡Œã—ã¦ãã‚Œã‚‹ã¿ãŸã„ã§ã™ã€‚ä¾‹ãˆã°è¤‡æ•°ãƒ¢ãƒ‡ãƒ«ã®ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã‚’ã™ã‚‹ã‚ˆã†ãªãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®å ´åˆã¯ã“ã¡ã‚‰ã§å®Ÿè¡Œã™ã‚Œã°å‡¦ç†ãŒæ—©ããªã‚Šãã†ã§ã™ã€‚

#### ThreadRunner
å…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã«ã‚‚ã‚ã¾ã‚Šå¤šãã®è¨˜è¿°ãŒç„¡ãã‚ˆãã‚ã‹ã‚‰ãªã‹ã£ãŸã®ã§ã™ãŒã€Apache Sparkã‚’ä½¿ç”¨ã™ã‚‹nodeã‚’çµ„ã¿è¾¼ã‚“ã pipelineã‚’å®Ÿè¡Œã™ã‚‹å ´åˆã€ParallelRunnerã‚’ä½¿ã†ã¨å®Ÿè¡Œé †åºãŒãŠã‹ã—ããªã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ãŸã‚ã€ThreadRunnerã‚’ä½¿ç”¨ã™ã‚‹å¿…è¦ãŒã‚ã‚‹ãã†ã§ã™ã€‚`kedro run --runner=ThreadRunner`ã§å®Ÿè¡Œã§ãã¾ã™ã€‚


## çµ‚ã‚ã‚Šã«
å°‘ã€…é•·ããªã£ã¦ã—ã¾ã„ã¾ã—ãŸã€‚ä»Šå›ã¯å˜ç´”ãªå‡¦ç†ã‚’è¡Œã†å ´åˆã®æ‰‹æ³•ã‚’æ›¸ãã¾ã—ãŸãŒã€kaggleã§ä½¿ã†æ™‚ã€å®Ÿéš›ã®ãƒ—ãƒ­ãƒ€ã‚¯ãƒˆã«çµ„ã¿è¾¼ã‚€æ™‚ã®Tipsç­‰ã€çŸ¥è¦‹ãŒæºœã¾ã£ãŸã‚‰æ›¸ã„ã¦ã„ããŸã„ãªã¨æ€ã„ã¾ã™ã€‚
